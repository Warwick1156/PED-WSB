{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from os import path\n",
    "import time \n",
    "from datetime import datetime \n",
    "import math\n",
    "\n",
    "import pickle\n",
    "\n",
    "data_dir = path.join('..', 'data')\n",
    "img_dir = path.join(data_dir, 'img')\n",
    "temp_dir = path.join(data_dir, 'temp')\n",
    "\n",
    "dataset_file = 'reddit_wsb_art.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path.join(data_dir, dataset_file))\n",
    "data = data.sort_values(by=['timestamp'])\n",
    "data.body = data.body.astype(str)\n",
    "data.title = data.title.astype(str)\n",
    "data.body = data.body.apply(lambda x: x.encode('utf-8').decode('unicode-escape'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>is_oc</th>\n",
       "      <th>permalink</th>\n",
       "      <th>name</th>\n",
       "      <th>is_self</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28052</th>\n",
       "      <td>MTCH IS A ONE WAY TICKET TO ORION'S BELT</td>\n",
       "      <td>9</td>\n",
       "      <td>l2a333</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>19</td>\n",
       "      <td>1.611270e+09</td>\n",
       "      <td>Unless mass castration becomes a popular trend...</td>\n",
       "      <td>Fri Jan 22 00:00:17 2021</td>\n",
       "      <td>0.61</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/l2a333/mtch_is_a_on...</td>\n",
       "      <td>t3_l2a333</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28937</th>\n",
       "      <td>Its no GME yolo but this is a pretty big short...</td>\n",
       "      <td>33</td>\n",
       "      <td>l2a5hd</td>\n",
       "      <td>https://i.redd.it/5brnguo1orc61.jpg</td>\n",
       "      <td>23</td>\n",
       "      <td>1.611270e+09</td>\n",
       "      <td>nan</td>\n",
       "      <td>Fri Jan 22 00:03:22 2021</td>\n",
       "      <td>0.79</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/l2a5hd/its_no_gme_y...</td>\n",
       "      <td>t3_l2a5hd</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8571</th>\n",
       "      <td>Gaybear Appreciation Post, I’m a naked Put sel...</td>\n",
       "      <td>32</td>\n",
       "      <td>l2a615</td>\n",
       "      <td>https://i.redd.it/empum556orc61.jpg</td>\n",
       "      <td>13</td>\n",
       "      <td>1.611270e+09</td>\n",
       "      <td>nan</td>\n",
       "      <td>Fri Jan 22 00:04:05 2021</td>\n",
       "      <td>0.87</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/l2a615/gaybear_appr...</td>\n",
       "      <td>t3_l2a615</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16672</th>\n",
       "      <td>Game-stop technical analysis. Know when to gam...</td>\n",
       "      <td>126</td>\n",
       "      <td>l2a742</td>\n",
       "      <td>https://i.redd.it/mbmpi88gorc61.jpg</td>\n",
       "      <td>42</td>\n",
       "      <td>1.611270e+09</td>\n",
       "      <td>nan</td>\n",
       "      <td>Fri Jan 22 00:05:39 2021</td>\n",
       "      <td>0.91</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/l2a742/gamestop_tec...</td>\n",
       "      <td>t3_l2a742</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7839</th>\n",
       "      <td>$SPCE is going to break out soon</td>\n",
       "      <td>132</td>\n",
       "      <td>l2a87f</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>52</td>\n",
       "      <td>1.611270e+09</td>\n",
       "      <td>Here are my ideas: cup and handle ready to go\\...</td>\n",
       "      <td>Fri Jan 22 00:07:08 2021</td>\n",
       "      <td>0.96</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/l2a87f/spce_is_goin...</td>\n",
       "      <td>t3_l2a87f</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35697</th>\n",
       "      <td>CBOE Short Interest Report 1/27</td>\n",
       "      <td>2</td>\n",
       "      <td>l6gxwq</td>\n",
       "      <td>https://www.cboe.com/us/equities/market_statis...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.611788e+09</td>\n",
       "      <td>nan</td>\n",
       "      <td>Wed Jan 27 23:59:34 2021</td>\n",
       "      <td>0.67</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/l6gxwq/cboe_short_i...</td>\n",
       "      <td>t3_l6gxwq</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35944</th>\n",
       "      <td>am i late on this?</td>\n",
       "      <td>8</td>\n",
       "      <td>l6gxx2</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>19</td>\n",
       "      <td>1.611788e+09</td>\n",
       "      <td>are there plans to hold more stocks, i got 300...</td>\n",
       "      <td>Wed Jan 27 23:59:35 2021</td>\n",
       "      <td>0.83</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/l6gxx2/am_i_late_on...</td>\n",
       "      <td>t3_l6gxx2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>HOLD THE LINE!!!</td>\n",
       "      <td>21</td>\n",
       "      <td>l6gxzl</td>\n",
       "      <td>https://i.redd.it/pbm0tkntgyd61.png</td>\n",
       "      <td>7</td>\n",
       "      <td>1.611788e+09</td>\n",
       "      <td>nan</td>\n",
       "      <td>Wed Jan 27 23:59:40 2021</td>\n",
       "      <td>0.96</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/l6gxzl/hold_the_line/</td>\n",
       "      <td>t3_l6gxzl</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33509</th>\n",
       "      <td>All in on NOK, BB, and my bb AMC. See you in t...</td>\n",
       "      <td>99</td>\n",
       "      <td>l6gy1h</td>\n",
       "      <td>https://i.redd.it/poggax5vgyd61.jpg</td>\n",
       "      <td>15</td>\n",
       "      <td>1.611788e+09</td>\n",
       "      <td>nan</td>\n",
       "      <td>Wed Jan 27 23:59:44 2021</td>\n",
       "      <td>0.80</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/l6gy1h/all_in_on_no...</td>\n",
       "      <td>t3_l6gy1h</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22154</th>\n",
       "      <td>Bye bye student loans!</td>\n",
       "      <td>135</td>\n",
       "      <td>l6gy22</td>\n",
       "      <td>https://i.redd.it/3usys7cvgyd61.jpg</td>\n",
       "      <td>17</td>\n",
       "      <td>1.611788e+09</td>\n",
       "      <td>nan</td>\n",
       "      <td>Wed Jan 27 23:59:45 2021</td>\n",
       "      <td>0.97</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/l6gy22/bye_bye_stud...</td>\n",
       "      <td>t3_l6gy22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38908 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  score      id  \\\n",
       "28052           MTCH IS A ONE WAY TICKET TO ORION'S BELT      9  l2a333   \n",
       "28937  Its no GME yolo but this is a pretty big short...     33  l2a5hd   \n",
       "8571   Gaybear Appreciation Post, I’m a naked Put sel...     32  l2a615   \n",
       "16672  Game-stop technical analysis. Know when to gam...    126  l2a742   \n",
       "7839                    $SPCE is going to break out soon    132  l2a87f   \n",
       "...                                                  ...    ...     ...   \n",
       "35697                    CBOE Short Interest Report 1/27      2  l6gxwq   \n",
       "35944                                 am i late on this?      8  l6gxx2   \n",
       "584                                     HOLD THE LINE!!!     21  l6gxzl   \n",
       "33509  All in on NOK, BB, and my bb AMC. See you in t...     99  l6gy1h   \n",
       "22154                             Bye bye student loans!    135  l6gy22   \n",
       "\n",
       "                                                     url  comms_num  \\\n",
       "28052  https://www.reddit.com/r/wallstreetbets/commen...         19   \n",
       "28937                https://i.redd.it/5brnguo1orc61.jpg         23   \n",
       "8571                 https://i.redd.it/empum556orc61.jpg         13   \n",
       "16672                https://i.redd.it/mbmpi88gorc61.jpg         42   \n",
       "7839   https://www.reddit.com/r/wallstreetbets/commen...         52   \n",
       "...                                                  ...        ...   \n",
       "35697  https://www.cboe.com/us/equities/market_statis...          5   \n",
       "35944  https://www.reddit.com/r/wallstreetbets/commen...         19   \n",
       "584                  https://i.redd.it/pbm0tkntgyd61.png          7   \n",
       "33509                https://i.redd.it/poggax5vgyd61.jpg         15   \n",
       "22154                https://i.redd.it/3usys7cvgyd61.jpg         17   \n",
       "\n",
       "            created                                               body  \\\n",
       "28052  1.611270e+09  Unless mass castration becomes a popular trend...   \n",
       "28937  1.611270e+09                                                nan   \n",
       "8571   1.611270e+09                                                nan   \n",
       "16672  1.611270e+09                                                nan   \n",
       "7839   1.611270e+09  Here are my ideas: cup and handle ready to go\\...   \n",
       "...             ...                                                ...   \n",
       "35697  1.611788e+09                                                nan   \n",
       "35944  1.611788e+09  are there plans to hold more stocks, i got 300...   \n",
       "584    1.611788e+09                                                nan   \n",
       "33509  1.611788e+09                                                nan   \n",
       "22154  1.611788e+09                                                nan   \n",
       "\n",
       "                      timestamp  upvote_ratio  is_oc  \\\n",
       "28052  Fri Jan 22 00:00:17 2021          0.61  False   \n",
       "28937  Fri Jan 22 00:03:22 2021          0.79  False   \n",
       "8571   Fri Jan 22 00:04:05 2021          0.87  False   \n",
       "16672  Fri Jan 22 00:05:39 2021          0.91  False   \n",
       "7839   Fri Jan 22 00:07:08 2021          0.96  False   \n",
       "...                         ...           ...    ...   \n",
       "35697  Wed Jan 27 23:59:34 2021          0.67  False   \n",
       "35944  Wed Jan 27 23:59:35 2021          0.83  False   \n",
       "584    Wed Jan 27 23:59:40 2021          0.96  False   \n",
       "33509  Wed Jan 27 23:59:44 2021          0.80  False   \n",
       "22154  Wed Jan 27 23:59:45 2021          0.97  False   \n",
       "\n",
       "                                               permalink       name  is_self  \n",
       "28052  /r/wallstreetbets/comments/l2a333/mtch_is_a_on...  t3_l2a333     True  \n",
       "28937  /r/wallstreetbets/comments/l2a5hd/its_no_gme_y...  t3_l2a5hd    False  \n",
       "8571   /r/wallstreetbets/comments/l2a615/gaybear_appr...  t3_l2a615    False  \n",
       "16672  /r/wallstreetbets/comments/l2a742/gamestop_tec...  t3_l2a742    False  \n",
       "7839   /r/wallstreetbets/comments/l2a87f/spce_is_goin...  t3_l2a87f     True  \n",
       "...                                                  ...        ...      ...  \n",
       "35697  /r/wallstreetbets/comments/l6gxwq/cboe_short_i...  t3_l6gxwq    False  \n",
       "35944  /r/wallstreetbets/comments/l6gxx2/am_i_late_on...  t3_l6gxx2     True  \n",
       "584     /r/wallstreetbets/comments/l6gxzl/hold_the_line/  t3_l6gxzl    False  \n",
       "33509  /r/wallstreetbets/comments/l6gy1h/all_in_on_no...  t3_l6gy1h    False  \n",
       "22154  /r/wallstreetbets/comments/l6gy22/bye_bye_stud...  t3_l6gy22    False  \n",
       "\n",
       "[38908 rows x 13 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_time(timestamps):\n",
    "    str_time = [x.split()[3] for x in timestamps]\n",
    "\n",
    "    splited_time = [x.split(':') for x in str_time]\n",
    "    float_time = [float(x[0] + '.' + x[1]) for x in splited_time]\n",
    "\n",
    "    sin_time = [np.sin(2 * np.pi * x / 23.59) for x in float_time]\n",
    "    cos_time = [np.cos(2 * np.pi * x / 23.59) for x in float_time]\n",
    "    \n",
    "    return sin_time, cos_time\n",
    "\n",
    "sin_time, cos_time = encode_time(data.timestamp.values)\n",
    "data['sin_time'] = sin_time\n",
    "data['cos_time'] = cos_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_date(unix_timestamps):\n",
    "    days_of_year = [time.localtime(x).tm_yday for x in unix_timestamps]\n",
    "    \n",
    "    sin_date = [np.sin(2 * np.pi * x / 365.0) for x in days_of_year]\n",
    "    cos_date = [np.cos(2 * np.pi * x / 365.0) for x in days_of_year]\n",
    "    \n",
    "    return sin_date, cos_date\n",
    "\n",
    "sin_date, cos_date = encode_date(data.created.values)\n",
    "\n",
    "data['sin_date'] = sin_date\n",
    "data['cos_date'] = cos_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_in_titles(df):\n",
    "    titles = df.title.values\n",
    "    len_words = [len(title.split()) for title in titles]\n",
    "    \n",
    "    df['words_in_titles'] = len_words\n",
    "    return df\n",
    "\n",
    "data = words_in_titles(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_in_bodies(df, skip_empty=False):\n",
    "    bodies = df.body.values\n",
    "    len_words = [len(str(body).split()) if str(body) != 'nan' else 0 for body in bodies]\n",
    "    \n",
    "    df['words_in_body'] = len_words\n",
    "    return df\n",
    "\n",
    "data = words_in_bodies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words_in_titles</th>\n",
       "      <th>words_in_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28052</th>\n",
       "      <td>9</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28937</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8571</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16672</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7839</th>\n",
       "      <td>7</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35697</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35944</th>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33509</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22154</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38908 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       words_in_titles  words_in_body\n",
       "28052                9            225\n",
       "28937               19              0\n",
       "8571                24              0\n",
       "16672               20              0\n",
       "7839                 7             84\n",
       "...                ...            ...\n",
       "35697                5              0\n",
       "35944                5             15\n",
       "584                  3              0\n",
       "33509               18              0\n",
       "22154                4              0\n",
       "\n",
       "[38908 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['words_in_titles', 'words_in_body']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentance = data.iloc[1, 0]\n",
    "# print(sentance)\n",
    "# out = ' '.join([lancaster.stem(word) for word in sentance.split()])\n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = str(text)\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    ret = [word for word in word_tokenize(text.lower()) if word.isalpha() and word not in stopwords_list]\n",
    "    return list(set(ret))\n",
    "\n",
    "def tokenize_title_body(df):\n",
    "    df['title_tokens'] = df.apply(lambda x: tokenize(x['title']), axis=1)\n",
    "    df['body_tokens']  = df.apply(lambda x: tokenize(x['body']), axis=1)\n",
    "    \n",
    "tokenize_title_body(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_(tokens):\n",
    "    return [porter.stem(word) for word in tokens]\n",
    "\n",
    "def stem_title_body(df):\n",
    "    df['title_stem_tokens'] = df.apply(lambda x: stem_(x['title_tokens']), axis=1)\n",
    "    df['body_stem_tokens']  = df.apply(lambda x: stem_(x['body_tokens']), axis=1)\n",
    "    \n",
    "stem_title_body(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(path.join(data_dir, \"data_temp.csv\"), sep='`', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_capital_letters(text):\n",
    "    text = str(text)\n",
    "    return sum([1 for char in text if 91 > ord(char) > 64])\n",
    "\n",
    "def capital_letters_ratio(text):\n",
    "    text = str(text)\n",
    "    alphacount = sum([1 for char in text if str(char).isalpha()])\n",
    "    if alphacount == 0:\n",
    "        return 0\n",
    "    return count_capital_letters(text) / alphacount \n",
    "\n",
    "data['title_capital_letters_count'] = data.apply(lambda x: count_capital_letters(x['title']), axis=1) \n",
    "data['title_capital_letters_ratio'] = data.apply(lambda x: capital_letters_ratio(x['title']), axis=1).apply(lambda x: np.around(x, 3))\n",
    "\n",
    "data['body_capital_letters_count'] = data.apply(lambda x: count_capital_letters(x['body']), axis=1) \n",
    "data['body_capital_letters_ratio'] = data.apply(lambda x: capital_letters_ratio(x['body']), axis=1).apply(lambda x: np.around(x, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "RE_HTTP = re.compile(\"http(s)?://[/\\.A-z0-9]+\")\n",
    "\n",
    "def detect_urls(text):\n",
    "    text = str(text)\n",
    "\n",
    "    return [str(x[1].group(0)) for x in enumerate(re.finditer(RE_HTTP, text))]\n",
    "\n",
    "data['body_url'] = data.apply(lambda x: detect_urls(x['body']), axis=1) \n",
    "data['body_urls_count'] = data['body_url'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emoji import UNICODE_EMOJI\n",
    "\n",
    "EMOJIS = set(UNICODE_EMOJI['en'].keys())\n",
    "\n",
    "def filter_emojis(text):\n",
    "    text = str(text)\n",
    "    res = []\n",
    "    for word in text.split(' '):\n",
    "        for char in word:\n",
    "            if char in EMOJIS:\n",
    "                res.append(char)\n",
    "                \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emojis_ratio(df_origin, df_emojis):\n",
    "    x = df_origin.apply(len)\n",
    "    x = np.where(x == 0, 1, x)\n",
    "    return (df_emojis.apply(len) / x).apply(lambda x: np.around(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title_emojis'] = data.apply(lambda x: filter_emojis(x['title']), axis=1)\n",
    "data['title_emoji_count'] = data['title_emojis'].apply(len)\n",
    "data['title_emojis_ratio'] = emojis_ratio(data['title'], data['title_emojis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['body'] = data['body'].astype(str)\n",
    "data['body_emojis'] = data.apply(lambda x: filter_emojis(x['body']), axis=1)\n",
    "data['body_emoji_count'] = data['body_emojis'].apply(len)\n",
    "data['body_emojis_ratio'] = emojis_ratio(data['body'], data['body_emojis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>title_emojis</th>\n",
       "      <th>title_emoji_count</th>\n",
       "      <th>title_emojis_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20773</th>\n",
       "      <td>Got too cocky after initial gme gains 😢</td>\n",
       "      <td>[😢]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>$RIDE I don’t need a therapist... I just need ...</td>\n",
       "      <td>[💎, 🖕, 🏽]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6344</th>\n",
       "      <td>To all the 🌈🐻 trying to keep us down today</td>\n",
       "      <td>[🌈, 🐻]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11108</th>\n",
       "      <td>AMD baby will grab more on any dip 😈 SU BAE sh...</td>\n",
       "      <td>[😈]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8914</th>\n",
       "      <td>Coverage of the GME action today 🚀 🌕</td>\n",
       "      <td>[🚀, 🌕]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26862</th>\n",
       "      <td>NOK 🚀🚀🚀🚀🚀</td>\n",
       "      <td>[🚀, 🚀, 🚀, 🚀, 🚀]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30006</th>\n",
       "      <td>BRAGG⬆️🚀</td>\n",
       "      <td>[⬆, 🚀]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17756</th>\n",
       "      <td>Until Discord server is back join to the new s...</td>\n",
       "      <td>[🚀, 🚀]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34490</th>\n",
       "      <td>Let's sent BB into Space. 🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀</td>\n",
       "      <td>[🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀]</td>\n",
       "      <td>15</td>\n",
       "      <td>0.366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25969</th>\n",
       "      <td>MRW I limit bought the GME $250 dip this morni...</td>\n",
       "      <td>[🚀, 🚀, 🚀]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7796 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "20773            Got too cocky after initial gme gains 😢   \n",
       "889    $RIDE I don’t need a therapist... I just need ...   \n",
       "6344          To all the 🌈🐻 trying to keep us down today   \n",
       "11108  AMD baby will grab more on any dip 😈 SU BAE sh...   \n",
       "8914                Coverage of the GME action today 🚀 🌕   \n",
       "...                                                  ...   \n",
       "26862                                          NOK 🚀🚀🚀🚀🚀   \n",
       "30006                                           BRAGG⬆️🚀   \n",
       "17756  Until Discord server is back join to the new s...   \n",
       "34490          Let's sent BB into Space. 🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀🚀   \n",
       "25969  MRW I limit bought the GME $250 dip this morni...   \n",
       "\n",
       "                                        title_emojis  title_emoji_count  \\\n",
       "20773                                            [😢]                  1   \n",
       "889                                        [💎, 🖕, 🏽]                  3   \n",
       "6344                                          [🌈, 🐻]                  2   \n",
       "11108                                            [😈]                  1   \n",
       "8914                                          [🚀, 🌕]                  2   \n",
       "...                                              ...                ...   \n",
       "26862                                [🚀, 🚀, 🚀, 🚀, 🚀]                  5   \n",
       "30006                                         [⬆, 🚀]                  2   \n",
       "17756                                         [🚀, 🚀]                  2   \n",
       "34490  [🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀]                 15   \n",
       "25969                                      [🚀, 🚀, 🚀]                  3   \n",
       "\n",
       "       title_emojis_ratio  \n",
       "20773               0.026  \n",
       "889                 0.040  \n",
       "6344                0.048  \n",
       "11108               0.016  \n",
       "8914                0.056  \n",
       "...                   ...  \n",
       "26862               0.556  \n",
       "30006               0.250  \n",
       "17756               0.037  \n",
       "34490               0.366  \n",
       "25969               0.025  \n",
       "\n",
       "[7796 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['title', 'title_emojis', 'title_emoji_count', 'title_emojis_ratio']].loc[data['title_emoji_count'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>body_emojis</th>\n",
       "      <th>body_emoji_count</th>\n",
       "      <th>body_emojis_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10187</th>\n",
       "      <td>I know the first thing that pops into all of o...</td>\n",
       "      <td>[🚀]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11161</th>\n",
       "      <td>First of all I'm in. I get the vision and I'm ...</td>\n",
       "      <td>[🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🥜, 🥜, 🥜, 🥜, 🥜, 🙌, ...</td>\n",
       "      <td>22</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27415</th>\n",
       "      <td>(Discretion: First post here. Go easy on me fe...</td>\n",
       "      <td>[🚀, 🚀, 🚀, 💩, 🚀, 🚀, 📈, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, ...</td>\n",
       "      <td>22</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14278</th>\n",
       "      <td>I’m currently holding 250 shares and hoping to...</td>\n",
       "      <td>[🚀, 🚀, 🚀, 🚀]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11956</th>\n",
       "      <td>Newer here, first time poster. I think I did t...</td>\n",
       "      <td>[🌈, 🐻]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985</th>\n",
       "      <td>Hello, you rich beautiful degenerates. \\n\\nLet...</td>\n",
       "      <td>[🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🔥, 🔥, 🔥]</td>\n",
       "      <td>9</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26862</th>\n",
       "      <td>I don’t have a lot of many but I bought 7 shar...</td>\n",
       "      <td>[🚀, 🚀, 🚀, 💎, 🚀, 🚀, 🚀, 🚀]</td>\n",
       "      <td>8</td>\n",
       "      <td>0.129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>Tl;dr: Fear was pushed hard today. But we push...</td>\n",
       "      <td>[🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30006</th>\n",
       "      <td>BRAGG - Under the Radar!  Next level gaming st...</td>\n",
       "      <td>[📡, 🔺, ⬆, 🚀, 🚀]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34490</th>\n",
       "      <td>We said GME and BB in the beginning so lets fi...</td>\n",
       "      <td>[🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀]</td>\n",
       "      <td>8</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4377 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    body  \\\n",
       "10187  I know the first thing that pops into all of o...   \n",
       "11161  First of all I'm in. I get the vision and I'm ...   \n",
       "27415  (Discretion: First post here. Go easy on me fe...   \n",
       "14278  I’m currently holding 250 shares and hoping to...   \n",
       "11956  Newer here, first time poster. I think I did t...   \n",
       "...                                                  ...   \n",
       "2985   Hello, you rich beautiful degenerates. \\n\\nLet...   \n",
       "26862  I don’t have a lot of many but I bought 7 shar...   \n",
       "1827   Tl;dr: Fear was pushed hard today. But we push...   \n",
       "30006  BRAGG - Under the Radar!  Next level gaming st...   \n",
       "34490  We said GME and BB in the beginning so lets fi...   \n",
       "\n",
       "                                             body_emojis  body_emoji_count  \\\n",
       "10187                                                [🚀]                 1   \n",
       "11161  [🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🥜, 🥜, 🥜, 🥜, 🥜, 🙌, ...                22   \n",
       "27415  [🚀, 🚀, 🚀, 💩, 🚀, 🚀, 📈, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, ...                22   \n",
       "14278                                       [🚀, 🚀, 🚀, 🚀]                 4   \n",
       "11956                                             [🌈, 🐻]                 2   \n",
       "...                                                  ...               ...   \n",
       "2985                         [🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🔥, 🔥, 🔥]                 9   \n",
       "26862                           [🚀, 🚀, 🚀, 💎, 🚀, 🚀, 🚀, 🚀]                 8   \n",
       "1827                      [🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀]                10   \n",
       "30006                                    [📡, 🔺, ⬆, 🚀, 🚀]                 5   \n",
       "34490                           [🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀, 🚀]                 8   \n",
       "\n",
       "       body_emojis_ratio  \n",
       "10187              0.000  \n",
       "11161              0.009  \n",
       "27415              0.005  \n",
       "14278              0.004  \n",
       "11956              0.001  \n",
       "...                  ...  \n",
       "2985               0.005  \n",
       "26862              0.129  \n",
       "1827               0.007  \n",
       "30006              0.086  \n",
       "34490              0.056  \n",
       "\n",
       "[4377 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['body', 'body_emojis', 'body_emoji_count', 'body_emojis_ratio']].loc[data['body_emoji_count'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('meme_ocr.pkl', 'rb') as file:\n",
    "    test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['image_text'] = test\n",
    "\n",
    "data.image_text = data.apply(lambda x: tokenize(x.image_text), axis=1)\n",
    "data.image_text = data.apply(lambda x: stem_(x.image_text), axis=1)\n",
    "\n",
    "data['image_text_words'] = data.apply(lambda x: len(x.image_text), axis=1)\n",
    "data['image_text_capital_letters_count'] = data.apply(lambda x: count_capital_letters(x.image_text), axis=1) \n",
    "data['image_text_capital_letters_ratio'] = data.apply(lambda x: capital_letters_ratio(x.image_text), axis=1).apply(lambda x: np.around(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_text</th>\n",
       "      <th>image_text_words</th>\n",
       "      <th>image_text_capital_letters_count</th>\n",
       "      <th>image_text_capital_letters_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11340</th>\n",
       "      <td>[per, portion, expens, tax, class, par, invent...</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21116</th>\n",
       "      <td>[help, track, pm, manufactur, last, thursday, ...</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37093</th>\n",
       "      <td>[pe, eer, sf, um, cover, ff, noth, lq, ef, aca...</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11956</th>\n",
       "      <td>[inseego, corp, eae, oe, pice, ba, c]</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4818</th>\n",
       "      <td>[dunn, smalicap, rebalanc, cover, estim, fund,...</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>[corp, stephen, chart, intern, blackston, tp, ...</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32720</th>\n",
       "      <td>[open, day, bbo, l, portfolio, avg, price, lim...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9453</th>\n",
       "      <td>[on, amount, invest, price, complet, order, gm...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16660</th>\n",
       "      <td>[open, music, live, zoom, get, happi, x, ke, r...</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30079</th>\n",
       "      <td>[gme, factor, unpreced, amid, compani, secur, ...</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>952 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              image_text  image_text_words  \\\n",
       "11340  [per, portion, expens, tax, class, par, invent...                67   \n",
       "21116  [help, track, pm, manufactur, last, thursday, ...                67   \n",
       "37093  [pe, eer, sf, um, cover, ff, noth, lq, ef, aca...                96   \n",
       "11956              [inseego, corp, eae, oe, pice, ba, c]                 7   \n",
       "4818   [dunn, smalicap, rebalanc, cover, estim, fund,...               202   \n",
       "...                                                  ...               ...   \n",
       "596    [corp, stephen, chart, intern, blackston, tp, ...                35   \n",
       "32720  [open, day, bbo, l, portfolio, avg, price, lim...                17   \n",
       "9453   [on, amount, invest, price, complet, order, gm...                17   \n",
       "16660  [open, music, live, zoom, get, happi, x, ke, r...                25   \n",
       "30079  [gme, factor, unpreced, amid, compani, secur, ...                35   \n",
       "\n",
       "       image_text_capital_letters_count  image_text_capital_letters_ratio  \n",
       "11340                                 0                               0.0  \n",
       "21116                                 0                               0.0  \n",
       "37093                                 0                               0.0  \n",
       "11956                                 0                               0.0  \n",
       "4818                                  0                               0.0  \n",
       "...                                 ...                               ...  \n",
       "596                                   0                               0.0  \n",
       "32720                                 0                               0.0  \n",
       "9453                                  0                               0.0  \n",
       "16660                                 0                               0.0  \n",
       "30079                                 0                               0.0  \n",
       "\n",
       "[952 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['image_text','image_text_words','image_text_capital_letters_count','image_text_capital_letters_ratio']].loc[data.image_text.apply(len).gt(0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HSV & Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels.h5\n",
      "574717952/574710816 [==============================] - 139s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from os import path\n",
    "import time \n",
    "from datetime import datetime \n",
    "import math\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "import cv2\n",
    "import mimetypes\n",
    "import requests\n",
    "import re\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "RE_HTTP = re.compile(\"http(s)?://[/\\.A-z0-9]+\")\n",
    "\n",
    "def detect_urls(text):\n",
    "    text = str(text)\n",
    "    return [str(x[1].group(0)) for x in enumerate(re.finditer(RE_HTTP, text))]\n",
    "\n",
    "data['body_url'] = data.apply(lambda x: detect_urls(x['body']), axis=1) \n",
    "data['body_urls_count'] = data['body_url'].apply(len)\n",
    "\n",
    "def is_url_image(url):    \n",
    "    mimetype,encoding = mimetypes.guess_type(url)\n",
    "    return (mimetype and mimetype.startswith('image'))\n",
    "\n",
    "def download_image(url, name):\n",
    "    try:\n",
    "        request = requests.get(url, stream = True)\n",
    "        status = request.status_code\n",
    "    except:\n",
    "        status = -1\n",
    "    \n",
    "    if status == 200:\n",
    "        with open(path.join(temp_dir, name), 'wb') as file:\n",
    "            file.write(request.content)\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_name(url):\n",
    "    return url.split('/')[-1]\n",
    "\n",
    "def correct_reddit_preview_url(url):\n",
    "    return url.replace('preview.redd.it', 'i.redd.it')\n",
    "\n",
    "from keras.applications.vgg19  import VGG19, preprocess_input, decode_predictions\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "import numpy as np\n",
    "from keras.applications.vgg19  import VGG19, preprocess_input, decode_predictions\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from skimage.color import rgb2hsv\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def get_img_avg_colors(image):\n",
    "\n",
    "    X = 0.0\n",
    "    Y = 0.0\n",
    "    count = 0\n",
    "    sat = 0\n",
    "    val = 0\n",
    "    \n",
    "    hsv = rgb2hsv(image)\n",
    "    for i in range(0,image.shape[0], 44):\n",
    "        for j in range(0,image.shape[1], 44):\n",
    "            X += math.cos(hsv[i,j,0] / 180.0 * math.pi)\n",
    "            Y += math.sin(hsv[i,j,0] / 180.0 * math.pi)\n",
    "            sat += hsv[i,j,1]\n",
    "            val += hsv[i,j,2]\n",
    "            count += 1\n",
    "\n",
    "    X /= count\n",
    "    Y /= count\n",
    "    avg_hue = math.atan2(Y, X) * 180.0 / math.pi;\n",
    "    avg_sat = sat / count\n",
    "    avg_val = val / count\n",
    "    return avg_hue, avg_sat, avg_val\n",
    "\n",
    "from keras.applications.vgg19  import VGG19, preprocess_input, decode_predictions\n",
    "from keras.preprocessing.image import img_to_array\n",
    "model = VGG19()\n",
    "\n",
    "def get_img_label(image):\n",
    "    \n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "    yhat  = model.predict(image)\n",
    "    label = decode_predictions(yhat, top=5)\n",
    "    label = label[0][0]\n",
    "    return label[1]\n",
    "\n",
    "def process_urls(url_list):\n",
    "    labels = []\n",
    "    colors = []\n",
    "    colors_stdevs = []\n",
    "    \n",
    "    for url in url_list:\n",
    "        if is_url_image(url):\n",
    "            name = get_name(url)\n",
    "            url = correct_reddit_preview_url(url)\n",
    "            \n",
    "            file_exists = False\n",
    "            path_to_file = os.path.join(temp_dir, name)\n",
    "            if os.path.exists(path_to_file):\n",
    "                file_exists = True\n",
    "            else:\n",
    "                download_successful = download_image(url, name)\n",
    "                file_exists = download_successful\n",
    "                \n",
    "            if file_exists:\n",
    "  \n",
    "                if path_to_file[-3:] == \"gif\":\n",
    "                    label = \"\"\n",
    "                    avg_col = (-1, 0, 0)\n",
    "                else:\n",
    "                    image = load_img(path_to_file, target_size=(224, 224))\n",
    "                    image = img_to_array(image)\n",
    "\n",
    "                    label = get_img_label(image)\n",
    "                    avg_col = get_img_avg_colors(image)\n",
    "\n",
    "                labels.append(label)\n",
    "                colors.append(avg_col)\n",
    "                \n",
    "    return labels, colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node vgg19/block1_conv1/Relu (defined at <ipython-input-27-4aabc24358c7>:96) ]] [Op:__inference_predict_function_704]\n\nFunction call stack:\npredict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-4298d8a89fe0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body_url'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprocess_urls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'imgs_labels'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'imgs_colors'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\techw\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3848\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-4298d8a89fe0>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body_url'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprocess_urls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'imgs_labels'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'imgs_colors'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-4aabc24358c7>\u001b[0m in \u001b[0;36mprocess_urls\u001b[1;34m(url_list)\u001b[0m\n\u001b[0;32m    126\u001b[0m                     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                     \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_img_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m                     \u001b[0mavg_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_img_avg_colors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-4aabc24358c7>\u001b[0m in \u001b[0;36mget_img_label\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m     \u001b[0myhat\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\techw\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1627\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1629\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\techw\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\techw\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    860\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mc:\\users\\techw\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\techw\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\users\\techw\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\techw\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node vgg19/block1_conv1/Relu (defined at <ipython-input-27-4aabc24358c7>:96) ]] [Op:__inference_predict_function_704]\n\nFunction call stack:\npredict_function\n"
     ]
    }
   ],
   "source": [
    "temp = data['body_url'].apply(lambda x: process_urls(x))\n",
    "data['imgs_labels'] = temp.apply(lambda x: x[0])\n",
    "data['imgs_colors'] = temp.apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['imgs_count'] = data['imgs_labels'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['body_url','imgs_labels','imgs_colors','imgs_count']].loc[data.imgs_count > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter(np.concatenate(data.imgs_labels.loc[data.imgs_labels.apply(len).gt(0)].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "c.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.copy()\n",
    "data2.body = data2.body.apply(lambda x: \" \".join(str.splitlines(x)))\n",
    "data2.to_csv(path.join(data_dir, \"merged_data5.csv\"), sep='`', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
