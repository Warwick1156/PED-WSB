{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\techw\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "c:\\users\\techw\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tqdm\\std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import ast\n",
    "\n",
    "from os import path\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "data_dir = path.join('..', 'data')\n",
    "img_dir = path.join(data_dir, 'img')\n",
    "temp_dir = path.join(data_dir, 'temp')\n",
    "\n",
    "dataset_file = 'merged_data4_with_sentiment.csv'\n",
    "\n",
    "data = pd.read_csv(path.join(data_dir, dataset_file), sep='`')\n",
    "data = data.sort_values(by=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hstack_title_body_tokens(title, body):\n",
    "    tokens = list(title)\n",
    "    filtered_body_tokens = [body_tokens for body_tokens in body if body_tokens != '[\\'nan\\']']\n",
    "\n",
    "    for list_ in filtered_body_tokens:\n",
    "        tokens.append(list_)\n",
    "\n",
    "    tokens = [ast.literal_eval(res) for res in tokens]\n",
    "    assert len(tokens) == len(filtered_body_tokens) + len(data.title_tokens.values)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = list(data.title_tokens.values)\n",
    "# filtered_body_tokens = [body_tokens for body_tokens in data.body_tokens.values if body_tokens != '[\\'nan\\']']\n",
    "\n",
    "# for list_ in filtered_body_tokens:\n",
    "#     tokens.append(list_)\n",
    "    \n",
    "# tokens = [ast.literal_eval(res) for res in tokens]\n",
    "# assert len(tokens) == len(filtered_body_tokens) + len(data.title_tokens.values)\n",
    "\n",
    "stem_tokens = hstack_title_body_tokens(data.title_tokens, data.body_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(tokens, vector_size=100):\n",
    "    start_time = time.time()\n",
    "    model = Word2Vec(\n",
    "                sentences=tokens, \n",
    "                sg=1, \n",
    "                vector_size=300,  \n",
    "                workers=4\n",
    "            )\n",
    "\n",
    "    print(f'Time taken : {(time.time() - start_time) / 60:.2f} mins')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, name):\n",
    "    model.wv.save_word2vec_format(path.join(data_dir, name))\n",
    "\n",
    "# save(model, 'wsb_embedding_300d.txt')\n",
    "# How to load:\n",
    "# w2v = KeyedVectors.load_word2vec_format(path.join(data_dir, 'wsb_embedding_300d.txt'))\n",
    "\n",
    "# How to get vector using loaded model\n",
    "# w2v.get_vector('gme')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv(path.join(data_dir, 'data_temp.csv'), sep='`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title_stem_tokens'] = temp.title_stem_tokens\n",
    "data['body_stem_tokens'] = temp.body_stem_tokens\n",
    "data['title_tokens'] = temp.title_tokens\n",
    "data['body_tokens'] = temp.body_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = hstack_title_body_tokens(data.title_tokens, data.body_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken : 0.29 mins\n",
      "Time taken : 0.22 mins\n"
     ]
    }
   ],
   "source": [
    "model = fit(tokens)\n",
    "save(model, 'wsb_embedding_100d.txt')\n",
    "\n",
    "stem_model = fit(stem_tokens)\n",
    "save(stem_model, 'wsb_stem_embedding_100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
